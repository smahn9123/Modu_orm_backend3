# 기본 명령어

```
mkdir 241230
cd 241230
python -m venv venv
.\venv\Scripts\Activate
pip install requests bs4
pip install fastapi uvicorn


# main.py 생성 및 아래 코드 작성
# from fastapi import FastAPI
# 
# app = FastAPI()
# 
# 
# @app.get("/")
# def read_root():
#     return {"Hello": "World"}


uvicorn main:app --reload

#####################################
# 아래 코드로 main.py를 수정해주세요.

from fastapi import FastAPI
import requests
from bs4 import BeautifulSoup

app = FastAPI()


@app.get("/")
def read_root():
    return {"Hello": "World"}


@app.get("/books")
def read_books():
    url = "https://paullab.co.kr/bookservice/"
    response = requests.get(url)

    soup = BeautifulSoup(response.text, "html.parser")

    class Book:
        def __init__(self, name, cover, author, price):
            self.name = name
            self.cover = cover
            self.author = author
            self.price = price

        def __repr__(self):
            return f"{self.name}"

        def __str__(self):
            return f"{self.name}"

        def to_html(self):
            return f'<h2>{self.name}</h2>\n<img src="{self.cover}">\n<p>{self.author}</p>\n<p>{self.price_comma()}</p>\n'

        def price_comma(self):
            return format(self.price, ",")

    data = []

    for name, cover, price, author in zip(
        soup.select(".book_name"),
        soup.select(".book_cover"),
        soup.select(".book_info")[::3],
        soup.select(".book_info")[1::3],
    ):
        if (
            price.text.replace("가격: ", "").replace("원", "").replace(",", "")
            == "무료"
        ):
            price = 0
        else:
            price = int(
                price.text.replace("가격: ", "").replace("원", "").replace(",", "")
            )
        data.append(
            Book(
                name.text,
                f'https://paullab.co.kr/bookservice/{cover["src"]}',
                author.text.replace("저자: ", ""),
                price,
            )
        )
    return [{"책 이름": book.name} for book in data]

```

# 내용

## 크롤링, 웹 스크래핑

- 크롤링, 웹 스크래핑: 웹 페이지를 그대로 가져와서(requests) 데이터를 추출(bs4)하는 기술
- Selenium 은 웹 페이지를 가져오는 것이 아니라 웹 브라우저를 띄워서 웹 페이지를 컨트롤 하는 기술입니다. 그런데 Selenium에다가 requests와 bs4를 합쳐서 사용할 수도 있습니다.
- pandas는 웹 페이지를 가져와서 추출하는 것까지 한꺼번에 가능, 다만 세부 컨트롤은 어려움
- 크롤링은 대부분 불법입니다. 웹 페이지 소유자의 허락을 받아야 합니다. (https://www.google.com/robots.txt 를 보시면 크롤링 허용 여부를 확인할 수 있습니다. 그러나 대부분 크롤링을 허용하고 있지 않습니다.)
- 실제 사례로 말씀을 드리자면 S그룹에서 K그룹(국가기관)에서 '서비스 정보보안 점검'이라는 이유로 대한민국 모든 주요 홈페이지를 크롤링 한적이 있었습니다. 크롤링을 돌리면 보통 여러대의 서비스로 정보를 수집하기 때문에 대상이 되는 서버입장에서는 DDoS 공격이 됩니다. 그래서 실제로 S그룹에 한 서비스가 죽었어요. 중소기업 서비는 특히나 취약합니다. 작은 트래픽에도 서비스가 죽을 수 있습니다.

```python
import requests
from bs4 import BeautifulSoup

url = 'https://paullab.co.kr/bookservice/'
response = requests.get(url)
# print(response.text)

######################################

soup = BeautifulSoup(response.text, 'html.parser')
# print(soup)

######################################

# print(soup.select('h2'))
# print(type(soup.select('h2')[0]))

######################################

# for i in soup.select('h2'):
#     print(i.text)

######################################

# for i in soup.select('div > .book_name'):
#     print(i.text)

######################################

# for i in soup.select('.book_cover'):
#     print('https://paullab.co.kr/bookservice/' + i['src'])

######################################

# for price, author in zip(soup.select('.book_info')[::3], soup.select('.book_info')[1::3]):
#     print(price.text)
#     print(author.text)
#     print('----')

######################################
# 크롤링한 데이터를 html 소스코드로 저장

s = ''

for name, cover in zip(soup.select('.book_name'), soup.select('.book_cover')):
    s += f'<h2>{name.text}</h2>\n'
    s += f'<img src="https://paullab.co.kr/bookservice/{cover["src"]}">\n'


with open('book.html', 'w', encoding='utf-8') as f:
    f.write(s)
```

# 과제
```
책 이름
책 커버
저자
가격
```
위 형식으로 크롤링 해서 html파일로 뽑아주세요.

```python
import requests
from bs4 import BeautifulSoup

url = 'https://paullab.co.kr/bookservice/'
response = requests.get(url)

soup = BeautifulSoup(response.text, 'html.parser')

# 리스트로 저장해도 되고, 딕셔너리로 저장해도 되지만
# 클래스로 저장하는 것이 가장 좋습니다. 
# 좋은 이유는 여러가지가 있지만, 가장 큰 이유는 데이터를 다루기 편하기 때문입니다.
# 예를 들어, print를 했을 때에도 깔끔하게 볼 수 있어요.
class Book:
    def __init__(self, name, cover, author, price):
        self.name = name
        self.cover = cover
        self.author = author
        self.price = price

    def __repr__(self):
        return f'{self.name}'

    def __str__(self):
        return f'{self.name}'

    def to_html(self):
        return f'<h2>{self.name}</h2>\n<img src="{self.cover}">\n<p>{self.author}</p>\n<p>{self.price_comma()}</p>\n'
    
    def price_comma(self):
        return format(self.price, ',')

data = []

for name, cover, price, author in zip(
    soup.select(".book_name"),
    soup.select(".book_cover"),
    soup.select(".book_info")[::3],
    soup.select(".book_info")[1::3],
):
    if price.text.replace('가격: ', '').replace('원', '').replace(',', '') == '무료':
        price = 0
    else:
        price = int(price.text.replace('가격: ', '').replace('원', '').replace(',', ''))
    data.append(
        Book(
            name.text,
            f'https://paullab.co.kr/bookservice/{cover["src"]}',
            author.text.replace('저자: ', ''),
            price,
        )
    )

s = ''
for i in data:
    s += i.to_html()

print(s)

with open('book.html', 'w', encoding='utf-8') as f:
    f.write(s)
```

# 기초지식
```python
class A:
    def __repr__(self):
        return 'hello world'

a = A()

[a, a, a]
```